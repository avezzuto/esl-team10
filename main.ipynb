{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f1d8e917fa49c2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T12:16:15.052720Z",
     "start_time": "2025-12-22T12:16:15.032572Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\greta\\esl-team10\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T12:16:15.461693Z",
     "start_time": "2025-12-22T12:16:15.067088Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\greta\\.cache\\kagglehub\\datasets\\nechbamohammed\\research-papers-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# Download dataset from Kaggle\n",
    "path = kagglehub.dataset_download(\"nechbamohammed/research-papers-dataset\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "739593bf7202b683",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T12:16:22.056046Z",
     "start_time": "2025-12-22T12:16:15.463580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dblp-v10.csv with shape (1000000, 8)\n"
     ]
    }
   ],
   "source": [
    "# Read dataset into pandas dataframe\n",
    "df = pd.read_csv(os.path.join(path, 'dblp-v10.csv'))\n",
    "print(f\"Loaded dblp-v10.csv with shape {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d28167abb90953d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T12:16:27.945541Z",
     "start_time": "2025-12-22T12:16:22.301561Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some preprocessing... make the authors of a paper into a list\n",
    "df['authors'] = df['authors'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71bdc80015854e76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T12:16:28.332653Z",
     "start_time": "2025-12-22T12:16:28.224794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After temporal filtering: 768113 papers\n"
     ]
    }
   ],
   "source": [
    "# We keep all papers until 2015. That way we can have a test set for papers from 2016 and 2017\n",
    "END_YEAR = 2015\n",
    "df = df[df['year'] <= END_YEAR]\n",
    "print(f\"After temporal filtering: {df.shape[0]} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3191e3a0c358e31b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T12:16:30.087518Z",
     "start_time": "2025-12-22T12:16:28.333608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eligible authors (>=2 papers): 319246\n",
      "After filtering for eligible authors: 735290 papers\n"
     ]
    }
   ],
   "source": [
    "# Count papers per author\n",
    "author_paper_count = defaultdict(int)\n",
    "for authors in df['authors']:\n",
    "    for a in authors:\n",
    "        author_paper_count[a] += 1\n",
    "\n",
    "# Using the count we keep only authors with >=2 papers to reduce graph size further\n",
    "eligible_authors = {a for a, c in author_paper_count.items() if c >= 2}\n",
    "print(f\"Eligible authors (>=2 papers): {len(eligible_authors)}\")\n",
    "\n",
    "def filter_authors(authors):\n",
    "    return [a for a in authors if a in eligible_authors]\n",
    "\n",
    "df['authors'] = df['authors'].apply(filter_authors)\n",
    "\n",
    "# Remove papers with <1 eligible author\n",
    "df = df[df['authors'].map(len) > 0]\n",
    "print(f\"After filtering for eligible authors: {df.shape[0]} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71dae111f90796ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T12:16:30.500781Z",
     "start_time": "2025-12-22T12:16:30.360427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After random sampling: 20000 papers\n"
     ]
    }
   ],
   "source": [
    "# We reduce the dataset further to 20000 randomly selected papers. Based on my testing, this is the max we can have before graph feature calculations explode in terms of time complexity\n",
    "TARGET_PAPERS = 20000\n",
    "if df.shape[0] > TARGET_PAPERS:\n",
    "    df = df.sample(n=TARGET_PAPERS, random_state=42).reset_index(drop=True)\n",
    "    print(f\"After random sampling: {df.shape[0]} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aaad0ba34fc13a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T12:16:30.542786Z",
     "start_time": "2025-12-22T12:16:30.508095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Carlos Castillo', 485),\n",
       " ('Z. Li', 50),\n",
       " ('Dereck S. Meek', 50),\n",
       " ('Desmond J. Walton', 50),\n",
       " ('Alexandre Demeure', 77),\n",
       " ('Jean-Sébastien Sottet', 77),\n",
       " ('Gaëlle Calvary', 82),\n",
       " ('Joëlle Coutaz', 77),\n",
       " ('Jean Vanderdonckt', 165),\n",
       " ('Giuseppe Iannaccone', 50)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map storing the number of citations for a particular author\n",
    "author_total_citations = defaultdict(int)\n",
    "\n",
    "for authors, cites in zip(df['authors'], df['n_citation']):\n",
    "    for a in authors:\n",
    "        author_total_citations[a] += cites\n",
    "\n",
    "list(author_total_citations.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44b6444f226ce5dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T12:16:30.607716Z",
     "start_time": "2025-12-22T12:16:30.544718Z"
    }
   },
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "# 1. Add nodes with attributes\n",
    "for a in author_total_citations:\n",
    "    G.add_node(a, total_citations=author_total_citations[a])\n",
    "\n",
    "# 2. Add edges between nodes\n",
    "for authors in df['authors']:\n",
    "    # Add an edge for all pairs of coauthors for this paper\n",
    "    for a1, a2 in combinations(authors, 2):\n",
    "        if G.has_edge(a1, a2):\n",
    "            G[a1][a2]['weight'] += 1\n",
    "        else:\n",
    "            G.add_edge(a1, a2, weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3323c560337c747",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T12:16:30.624970Z",
     "start_time": "2025-12-22T12:16:30.608091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors (nodes): 43325\n",
      "Coauthor edges: 65588\n"
     ]
    }
   ],
   "source": [
    "print(\"Authors (nodes):\", G.number_of_nodes())\n",
    "print(\"Coauthor edges:\", G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6775bdf4a3a94a4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T12:16:30.698646Z",
     "start_time": "2025-12-22T12:16:30.626091Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prestige of an author in the coauthor network\n",
    "pagerank = nx.pagerank(G, weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e3832c14e263921",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T12:16:39.751158Z",
     "start_time": "2025-12-22T12:16:30.699192Z"
    }
   },
   "outputs": [],
   "source": [
    "# How much an author bridges different groups\n",
    "betweenness = nx.betweenness_centrality(G, k = 500, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48792f8634f72f1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T12:16:58.752029Z",
     "start_time": "2025-12-22T12:16:40.045704Z"
    }
   },
   "outputs": [],
   "source": [
    "# How central an author is to the network\n",
    "closeness = nx.closeness_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "349e6c4c885ab0b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T12:16:59.448576Z",
     "start_time": "2025-12-22T12:16:59.047871Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fraction of the author’s coauthors who have also collaborated with each other (group tightness)\n",
    "clustering = nx.clustering(G, weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d23642a47735639c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T12:16:59.457806Z",
     "start_time": "2025-12-22T12:16:59.449511Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute mean citations per venue\n",
    "venue_mean_citations = df.groupby('venue')['n_citation'].mean().to_dict()\n",
    "\n",
    "# Compute total papers per venue\n",
    "venue_paper_count = df.groupby('venue').size().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f613f08f9028f002",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T12:17:15.812600Z",
     "start_time": "2025-12-22T12:16:59.458558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_pagerank</th>\n",
       "      <th>max_pagerank</th>\n",
       "      <th>mean_betweenness</th>\n",
       "      <th>max_betweenness</th>\n",
       "      <th>mean_closeness</th>\n",
       "      <th>max_closeness</th>\n",
       "      <th>mean_component_size</th>\n",
       "      <th>max_component_size</th>\n",
       "      <th>mean_weighted_degree</th>\n",
       "      <th>max_weighted_degree</th>\n",
       "      <th>mean_clustering</th>\n",
       "      <th>max_clustering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.074333</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_pagerank  max_pagerank  mean_betweenness  max_betweenness  \\\n",
       "0       0.000054      0.000054               0.0              0.0   \n",
       "1       0.000024      0.000024               0.0              0.0   \n",
       "2       0.000035      0.000059               0.0              0.0   \n",
       "3       0.000004      0.000004               0.0              0.0   \n",
       "4       0.000024      0.000024               0.0              0.0   \n",
       "\n",
       "   mean_closeness  max_closeness  mean_component_size  max_component_size  \\\n",
       "0        0.000115       0.000115                  6.0                 6.0   \n",
       "1        0.000046       0.000046                  3.0                 3.0   \n",
       "2        0.000165       0.000208                 13.0                13.0   \n",
       "3        0.000000       0.000000                  1.0                 1.0   \n",
       "4        0.000023       0.000023                  2.0                 2.0   \n",
       "\n",
       "   mean_weighted_degree  max_weighted_degree  mean_clustering  max_clustering  \n",
       "0                   5.0                  5.0         0.020000            0.02  \n",
       "1                   2.0                  2.0         0.100000            0.10  \n",
       "2                   5.2                  8.0         0.074333            0.10  \n",
       "3                   0.0                  0.0         0.000000            0.00  \n",
       "4                   1.0                  1.0         0.000000            0.00  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#==============================\n",
    "# GRAPH-BASED FEATURES PER AUTHOR\n",
    "#==============================\n",
    "\n",
    "# Node-level features dictionary\n",
    "author_features = {}\n",
    "for a in G.nodes():\n",
    "    author_features[a] = {\n",
    "        'pagerank': pagerank[a],\n",
    "        'betweenness': betweenness[a],\n",
    "        'closeness': closeness[a],\n",
    "        'component_size': len(nx.node_connected_component(G, a)),\n",
    "        'weighted_degree': sum(d['weight'] for _, _, d in G.edges(a, data=True)),\n",
    "        'clustering': clustering[a],\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# author_features['Some Author']['pagerank'] gives PageRank\n",
    "# author_features['Some Author']['degree'] gives number of coauthors\n",
    "\n",
    "#==============================\n",
    "# PAPER-LEVEL GRAPH FEATURES\n",
    "#==============================\n",
    "# Aggregate author-level features per paper\n",
    "def paper_graph_features(authors):\n",
    "    known_authors = [a for a in authors if a in author_features]\n",
    "\n",
    "    # aggregate stats per paper\n",
    "    pr = np.array([author_features[a]['pagerank'] for a in known_authors])\n",
    "    bt = np.array([author_features[a]['betweenness'] for a in known_authors])\n",
    "    cl = np.array([author_features[a]['closeness'] for a in known_authors])\n",
    "    cs = np.array([author_features[a]['component_size'] for a in known_authors])\n",
    "    wdeg = np.array([author_features[a]['weighted_degree'] for a in known_authors])\n",
    "    ct = np.array([author_features[a]['clustering'] for a in known_authors])\n",
    "\n",
    "    return {\n",
    "        'mean_pagerank': pr.mean() if len(pr) > 0 else 0,\n",
    "        'max_pagerank': pr.max() if len(pr) > 0 else 0,\n",
    "        'mean_betweenness': bt.mean() if len(bt) > 0 else 0,\n",
    "        'max_betweenness': bt.max() if len(bt) > 0 else 0,\n",
    "        'mean_closeness': cl.mean() if len(cl) > 0 else 0,\n",
    "        'max_closeness': cl.max() if len(cl) > 0 else 0,\n",
    "        'mean_component_size': cs.mean() if len(cs) > 0 else 0,\n",
    "        'max_component_size': cs.max() if len(cs) > 0 else 0,\n",
    "        'mean_weighted_degree': wdeg.mean() if len(wdeg) > 0 else 0,\n",
    "        'max_weighted_degree': wdeg.max() if len(wdeg) > 0 else 0,\n",
    "        'mean_clustering': ct.mean() if len(ct) > 0 else 0,\n",
    "        'max_clustering': ct.max() if len(ct) > 0 else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply to all papers\n",
    "graph_features_df = df['authors'].apply(paper_graph_features).apply(pd.Series)\n",
    "graph_features_df.head()\n",
    "\n",
    "# Example:\n",
    "# graph_features_df.iloc[0]['mean_pagerank'] gives average prestige of first paper's authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb848dbe49c83dcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T12:17:16.181783Z",
     "start_time": "2025-12-22T12:17:16.108373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors with non-zero betweenness: 2568 / 43325\n",
      "Fraction: 0.0593\n"
     ]
    }
   ],
   "source": [
    "# Count authors with non-zero betweenness. This feature has low variance but high meaningfulness when authors do diverge from the mean\n",
    "num_nonzero_bt = sum(1 for v in betweenness.values() if v > 0)\n",
    "\n",
    "num_authors = len(betweenness)\n",
    "\n",
    "print(f\"Authors with non-zero betweenness: {num_nonzero_bt} / {num_authors}\")\n",
    "print(f\"Fraction: {num_nonzero_bt / num_authors:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ebdbe3739702b1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T12:17:17.236022Z",
     "start_time": "2025-12-22T12:17:16.269252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_num_papers</th>\n",
       "      <th>max_num_papers</th>\n",
       "      <th>mean_total_citations</th>\n",
       "      <th>max_total_citations</th>\n",
       "      <th>sum_total_citations</th>\n",
       "      <th>venue_mean_citations</th>\n",
       "      <th>venue_num_papers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>180.076923</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>95.6</td>\n",
       "      <td>165.0</td>\n",
       "      <td>478.0</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>39.666667</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_num_papers  max_num_papers  mean_total_citations  max_total_citations  \\\n",
       "0              4.0             4.0                 485.0                485.0   \n",
       "1              1.0             1.0                  50.0                 50.0   \n",
       "2              1.8             4.0                  95.6                165.0   \n",
       "3              1.0             1.0                  50.0                 50.0   \n",
       "4              1.0             1.0                   1.0                  1.0   \n",
       "\n",
       "   sum_total_citations  venue_mean_citations  venue_num_papers  \n",
       "0                485.0            180.076923              13.0  \n",
       "1                150.0             53.000000               5.0  \n",
       "2                478.0             52.500000               2.0  \n",
       "3                 50.0             39.666667               9.0  \n",
       "4                  2.0             24.000000              17.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#==============================\n",
    "# NON-GRAPH FEATURES PER PAPER\n",
    "#==============================\n",
    "# - num_papers: how many papers an author has published\n",
    "# - total_citations: total citations of an author\n",
    "# - citations_per_paper: average citations per paper\n",
    "# - venue: statistics on a particular venue (mean citations per paper at the venue, total number of papers at the venue)\n",
    "author_papers_count = defaultdict(int)\n",
    "author_citations_total = defaultdict(int)\n",
    "for authors, cites in zip(df['authors'], df['n_citation']):\n",
    "    for a in authors:\n",
    "        author_papers_count[a] += 1\n",
    "        author_citations_total[a] += cites\n",
    "\n",
    "# Paper-level aggregation\n",
    "def paper_non_graph_features(authors, venue):\n",
    "    counts = [author_papers_count.get(a, 0) for a in authors]\n",
    "    citations = [author_citations_total.get(a, 0) for a in authors]\n",
    "    mean_cites_venue = venue_mean_citations.get(venue, np.nan)\n",
    "    num_papers_venue = venue_paper_count.get(venue, 0)\n",
    "\n",
    "    return {\n",
    "        'mean_num_papers': np.mean(counts) if counts else 0,\n",
    "        'max_num_papers': np.max(counts) if counts else 0,\n",
    "        'mean_total_citations': np.mean(citations) if citations else 0,\n",
    "        'max_total_citations': np.max(citations) if citations else 0,\n",
    "        'sum_total_citations': np.sum(citations) if citations else 0,\n",
    "        'venue_mean_citations': mean_cites_venue,\n",
    "        'venue_num_papers': num_papers_venue\n",
    "    }\n",
    "\n",
    "non_graph_features_df = df.apply(\n",
    "    lambda row: paper_non_graph_features(row['authors'], row['venue']), axis=1\n",
    ").apply(pd.Series)\n",
    "non_graph_features_df.head()\n",
    "\n",
    "# Example:\n",
    "# non_graph_features_df.iloc[0]['mean_num_papers'] is average productivity of authors of first paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "776981edcbf814a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T12:17:17.697004Z",
     "start_time": "2025-12-22T12:17:17.679533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph features shape: (20000, 12)\n",
      "Non-graph features shape: (20000, 7)\n",
      "Combined features shape: (20000, 19)\n"
     ]
    }
   ],
   "source": [
    "# Combined features and ground truths ready to be inputted\n",
    "X_graph = graph_features_df\n",
    "X_non_graph = non_graph_features_df\n",
    "X_all = pd.concat([graph_features_df, non_graph_features_df], axis=1)\n",
    "y = df['n_citation']\n",
    "\n",
    "print(\"Graph features shape:\", X_graph.shape)\n",
    "print(\"Non-graph features shape:\", X_non_graph.shape)\n",
    "print(\"Combined features shape:\", X_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75cd86d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ML libraries for model fitting\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9baccbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pipelines defined successfully\n"
     ]
    }
   ],
   "source": [
    "#===================\n",
    "# MODEL DEFINITIONS\n",
    "#===================\n",
    "\n",
    "# Define feature sets for comparison\n",
    "feature_sets = {\n",
    "    'Graph Features': X_graph,\n",
    "    'Non-Graph Features': X_non_graph,\n",
    "    'All Features': X_all\n",
    "}\n",
    "\n",
    "# Define model pipelines (imputation -> scaling -> model)\n",
    "def create_ridge_pipeline(alpha=1.0):\n",
    "    return Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', Ridge(alpha=alpha, random_state=42))\n",
    "    ])\n",
    "\n",
    "def create_rf_pipeline(n_estimators=100, max_depth=15):\n",
    "    return Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('model', RandomForestRegressor(\n",
    "            n_estimators=n_estimators, \n",
    "            max_depth=max_depth,\n",
    "            min_samples_leaf=5,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "def create_mlp_pipeline(hidden_layers=(64, 32)):\n",
    "    return Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', MLPRegressor(\n",
    "            hidden_layer_sizes=hidden_layers,\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            alpha=0.001,  # L2 regularization\n",
    "            max_iter=500,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.1,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "models = {\n",
    "    'Ridge Regression': create_ridge_pipeline,\n",
    "    'Random Forest': create_rf_pipeline,\n",
    "    'MLP': create_mlp_pipeline\n",
    "}\n",
    "\n",
    "print(\"Model pipelines defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "501a326f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting: Ridge Regression\n",
      " + Graph Features\n",
      " + Non-Graph Features\n",
      " + All Features\n",
      "\n",
      "Fitting: Random Forest\n",
      " + Graph Features\n",
      " + Non-Graph Features\n",
      " + All Features\n",
      "\n",
      "Fitting: MLP\n",
      " + Graph Features\n",
      " + Non-Graph Features\n",
      " + All Features\n",
      "\n",
      "9 models fitted successfully\n",
      "Access with: fitted_models[('Model Name', 'Feature Set')]\n"
     ]
    }
   ],
   "source": [
    "#================================\n",
    "# FIT MODELS ON ALL FEATURE SETS\n",
    "#================================\n",
    "\n",
    "fitted_models = {}\n",
    "\n",
    "for model_name, model_factory in models.items():\n",
    "    print(f\"\\nFitting: {model_name}\")\n",
    "    \n",
    "    for feature_name, X in feature_sets.items():\n",
    "        # Create and fit model\n",
    "        model = model_factory()\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Store fitted model\n",
    "        key = (model_name, feature_name)\n",
    "        fitted_models[key] = model\n",
    "        \n",
    "        print(f\" + {feature_name}\")\n",
    "\n",
    "print(f\"\\n{len(fitted_models)} models fitted successfully\")\n",
    "print(\"Access with: fitted_models[('Model Name', 'Feature Set')]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
